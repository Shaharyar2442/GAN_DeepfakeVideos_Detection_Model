{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9b551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if input directory exists: E:\\FakeAvCeleb\\FakeAVCeleb_v1.2\\FakeAVCeleb_v1.2\n",
      "Input directory exists: True\n",
      "Contents of input directory:\n",
      "- FakeVideo-FakeAudio\n",
      "- FakeVideo-RealAudio\n",
      "- meta_data.csv\n",
      "- README.txt\n",
      "- RealVideo-RealAudio\n",
      "Setup complete. Ready to process FakeAVCeleb.\n",
      "Setup complete. Ready to process FakeAVCeleb.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "input_base_dir = Path(r\"E:\\FakeAvCeleb\\FakeAVCeleb_v1.2\\FakeAVCeleb_v1.2\")\n",
    "\n",
    "print(f\"Checking if input directory exists: {input_base_dir}\")\n",
    "print(f\"Input directory exists: {input_base_dir.exists()}\")\n",
    "if input_base_dir.exists():\n",
    "    print(\"Contents of input directory:\")\n",
    "    for item in input_base_dir.iterdir():\n",
    "        print(f\"- {item.name}\")\n",
    "\n",
    "output_base_dir = Path(r\"E:\\Processed_FakeAVCeleb\")\n",
    "\n",
    "#Definine the source folders and their corresponding output labels (0=real, 1=fake)\n",
    "SOURCE_FOLDERS = [\n",
    "    (\"RealVideo-RealAudio\", \"0_real\"),\n",
    "    (\"FakeVideo-RealAudio\", \"1_fake\")\n",
    "]\n",
    "\n",
    "# List of video file extensions\n",
    "video_extensions = {\".mp4\", \".avi\", \".mov\", \".mkv\", \".flv\", \".wmv\"}\n",
    "\n",
    "# --- Parameters ---\n",
    "motion_threshold = 0.2 # Threshold for motion detection originaly was 0.7\n",
    "output_face_size = (256, 256)\n",
    "\n",
    "\n",
    "MIN_FRAMES_PER_VIDEO = 24 \n",
    "MAX_FRAMES_PER_VIDEO = 24 \n",
    "\n",
    "# --- Initializations ---\n",
    "detector = MTCNN()\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) # Suppress MTCNN FutureWarnings\n",
    "print(\"Setup complete. Ready to process FakeAVCeleb.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample frames based on optical flow \n",
    "def sample_frames_with_optical_flow(video_path, threshold):\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file at {video_path}\")\n",
    "        return []\n",
    "    \n",
    "    selected_frames = []\n",
    "    #Reading the first frame and conversion to grayscale\n",
    "\n",
    "    ret, prev_frame = cap.read() #ret= True if frame is read correctly\n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read the first frame of {video_path}\")\n",
    "        return []\n",
    "        \n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    selected_frames.append(prev_frame) # Adding the first frame to our list by default\n",
    "\n",
    "    while True:\n",
    "        # Reading the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # End of video\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Calculating dense optical flow between previous and current frame\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0) #Dense optical flow is calculated using Farneback method which gives vector field representing motion between two frames\n",
    "        \n",
    "        # Calculate the magnitude of the flow vectors and angle so that we can assess motion\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1]) #cv2.cartToPolar computes the magnitude and angle of 2D vectors\n",
    "        \n",
    "        # Calculating the average motion score for the frame\n",
    "        mean_magnitude = np.mean(magnitude)\n",
    "        \n",
    "        # If motion is above the threshold, keep the frame\n",
    "        if mean_magnitude > threshold:\n",
    "            selected_frames.append(frame)\n",
    "        \n",
    "        prev_gray = gray\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    return selected_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_crop_faces(frames, size): #Detects, aligns, and crops faces from a list of frames using MTCNN.\n",
    "    \n",
    "    processed_faces = []\n",
    "    \n",
    "    for frame in frames:\n",
    "        # MTCNN expects RGB format so convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect faces\n",
    "        results = detector.detect_faces(frame_rgb)\n",
    "        \n",
    "        #  We only process  if one confident face is found\n",
    "        if len(results) == 1:\n",
    "            keypoints = results[0]['keypoints']\n",
    "            left_eye = keypoints['left_eye']\n",
    "            right_eye = keypoints['right_eye']\n",
    "            \n",
    "            #Alignment is done based on eye positions \n",
    "            dY = right_eye[1] - left_eye[1] #Vertical distance between eyes\n",
    "            dX = right_eye[0] - left_eye[0] #Horizontal distance between eyes\n",
    "            angle = np.degrees(np.arctan2(dY, dX)) #Calculate angle between eyes in degrees\n",
    "            \n",
    "            (h, w) = frame.shape[:2] #Getting frame dimensions\n",
    "            center = (w // 2, h // 2)\n",
    "            \n",
    "            M = cv2.getRotationMatrix2D(center, angle, 1.0) #Get rotation matrix for rotating the image around its center by the calculated angle\n",
    "            rotated_frame = cv2.warpAffine(frame, M, (w, h), flags=cv2.INTER_CUBIC) #Rotating the frame to align eyes horizontally\n",
    "            #cv2.warpaffine works by applying an affine transformation to the image using the rotation matrix M\n",
    "            \n",
    "            #  Re-detecting face for Accurate Cropping \n",
    "            rotated_frame_rgb = cv2.cvtColor(rotated_frame, cv2.COLOR_BGR2RGB)\n",
    "            new_results = detector.detect_faces(rotated_frame_rgb)\n",
    "            \n",
    "            if len(new_results) == 1:\n",
    "                #Get bounding box of the detected face\n",
    "                x, y, width, height = new_results[0]['box']\n",
    "                # Add boundary checks for safety to ensure we don't go out of image bounds\n",
    "                y1, y2 = max(0, y), min(rotated_frame.shape[0], y + height)\n",
    "                x1, x2 = max(0, x), min(rotated_frame.shape[1], x + width)\n",
    "                #Cropping the face from the rotated frame\n",
    "                face_crop = rotated_frame[y1:y2, x1:x2]\n",
    "                #Only process if the cropped face is valid\n",
    "                if face_crop.size > 0:\n",
    "                    resized_face = cv2.resize(face_crop, size, interpolation=cv2.INTER_AREA)\n",
    "                    processed_faces.append(resized_face)\n",
    "                    \n",
    "    return processed_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e339c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_frames(faces, output_folder, unique_video_name):\n",
    "    \"\"\"\n",
    "    Saves a list of face images to a target directory.\n",
    "    Each video's frames will be in its own subdirectory.\n",
    "    \n",
    "    Args:\n",
    "        faces (list): A list of processed face images (numpy arrays).\n",
    "        output_folder (Path): The base output directory (e.g., .../0_real or .../1_fake).\n",
    "        unique_video_name (str): The unique name for the video folder (e.g., \"id00076_00109_1\").\n",
    "    \"\"\"\n",
    "    # Create a specific folder for this video's frames\n",
    "    video_output_folder = output_folder / unique_video_name\n",
    "    video_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save each frame in the video-specific folder\n",
    "    for i, face in enumerate(faces):\n",
    "        filename = f\"{i:04d}.png\"\n",
    "        save_path = video_output_folder / filename\n",
    "        cv2.imwrite(str(save_path), face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe9444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FakeAVCeleb processing...\n",
      "Frame selection policy: MIN 5, MAX 12 (based on motion threshold 0.7)\n",
      "--- Processing source folder: RealVideo-RealAudio (Saving to E:\\Processed_FakeAVCeleb\\0_real) ---\n",
      "--- Processing source folder: FakeVideo-RealAudio (Saving to E:\\Processed_FakeAVCeleb\\1_fake) ---\n",
      "--- Processing source folder: FakeVideo-RealAudio (Saving to E:\\Processed_FakeAVCeleb\\1_fake) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m \u001b[38;5;66;03m# Skip this video\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Step 2: Align faces\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m faces = \u001b[43malign_and_crop_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_face_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# --- NEW: Check MIN face count (in case some frames fail detection) ---\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) < MIN_FRAMES_PER_VIDEO:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# print(f\"Skipping {video_file.name}: only {len(faces)} faces detected. (Min: {MIN_FRAMES_PER_VIDEO})\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36malign_and_crop_faces\u001b[39m\u001b[34m(frames, size)\u001b[39m\n\u001b[32m      9\u001b[39m frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m results = \u001b[43mdetector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Process only if one confident face is found\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mtcnn\\mtcnn.py:159\u001b[39m, in \u001b[36mMTCNN.detect_faces\u001b[39m\u001b[34m(self, image, fit_to_image, limit_boundaries_landmarks, box_format, output_type, postprocess, batch_stack_justification, **kwargs)\u001b[39m\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Process images through each stage (PNet, RNet, ONet)\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stages:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m         bboxes_batch = \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbboxes_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbboxes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_normalized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_oshapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages_oshapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m tf.errors.InvalidArgumentError:  \u001b[38;5;66;03m# No faces found\u001b[39;00m\n\u001b[32m    162\u001b[39m     bboxes_batch = np.empty((\u001b[32m0\u001b[39m, \u001b[32m16\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mtcnn\\stages\\stage_pnet.py:92\u001b[39m, in \u001b[36mStagePNet.__call__\u001b[39m\u001b[34m(self, images_normalized, images_oshapes, min_face_size, min_size, scale_factor, threshold_pnet, nms_pnet1, nms_pnet2, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m batch_size = images_normalized.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# 3. Get proposals bounding boxes and confidence from the model (PNet)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m pnet_result = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scales_result]\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# 4. Generate bounding boxes per scale group\u001b[39;00m\n\u001b[32m     95\u001b[39m bboxes_proposals = [generate_bounding_box(result[\u001b[32m0\u001b[39m], result[\u001b[32m1\u001b[39m], threshold_pnet) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m pnet_result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\layer.py:936\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    934\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m936\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    940\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\operation.py:58\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     54\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     55\u001b[39m         call_fn,\n\u001b[32m     56\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     57\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\mtcnn\\network\\pnet.py:95\u001b[39m, in \u001b[36mPNet.call\u001b[39m\u001b[34m(self, inputs, *args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Third conv block\u001b[39;00m\n\u001b[32m     94\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv3(x)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprelu3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Outputs\u001b[39;00m\n\u001b[32m     98\u001b[39m bbox_reg = \u001b[38;5;28mself\u001b[39m.conv4_1(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\layer.py:936\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    934\u001b[39m         outputs = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m936\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[32m    940\u001b[39m distribution = distribution_lib.distribution()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\ops\\operation.py:58\u001b[39m, in \u001b[36mOperation.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     53\u001b[39m                 call_fn = \u001b[38;5;28mself\u001b[39m.call\n\u001b[32m     54\u001b[39m     call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[32m     55\u001b[39m         call_fn,\n\u001b[32m     56\u001b[39m         object_name=(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.call()\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     57\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[39m, in \u001b[36minject_argument_info_in_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m bound_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33m_keras_call_info_injected\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    159\u001b[39m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\activations\\prelu.py:76\u001b[39m, in \u001b[36mPReLU.call\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[32m     75\u001b[39m     pos = activations.relu(inputs)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     neg = \u001b[43m-\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m * activations.relu(-inputs)\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pos + neg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\backend\\common\\variables.py:446\u001b[39m, in \u001b[36mVariable.__neg__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__neg__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__neg__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\ops\\variables.py:1016\u001b[39m, in \u001b[36mVariable._OverloadOperator.<locals>._run_op\u001b[39m\u001b[34m(a, *args, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_op\u001b[39m(a, *args, **kwargs):\n\u001b[32m   1015\u001b[39m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor_oper(\u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:658\u001b[39m, in \u001b[36mBaseResourceVariable.value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_value\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28;01mNone\u001b[39;00m, ignore_existing=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:843\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op\u001b[39m\u001b[34m(self, no_copy)\u001b[39m\n\u001b[32m    841\u001b[39m       result = read_and_set_handle(no_copy)\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m   result = \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context.executing_eagerly():\n\u001b[32m    846\u001b[39m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[32m    847\u001b[39m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[32m    848\u001b[39m   record.record_operation(\n\u001b[32m    849\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mReadVariableOp\u001b[39m\u001b[33m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m.handle],\n\u001b[32m    850\u001b[39m       backward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[32m    851\u001b[39m       forward_function=\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:833\u001b[39m, in \u001b[36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[39m\u001b[34m(no_copy)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_copy \u001b[38;5;129;01mand\u001b[39;00m forward_compat.forward_compatible(\u001b[32m2022\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m3\u001b[39m):\n\u001b[32m    832\u001b[39m   gen_resource_variable_ops.disable_copy_on_read(\u001b[38;5;28mself\u001b[39m.handle)\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m result = \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m._dtype, \u001b[38;5;28mself\u001b[39m.handle, result)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:534\u001b[39m, in \u001b[36mread_variable_op\u001b[39m\u001b[34m(resource, dtype, name)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m    533\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReadVariableOp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    537\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- Main processing loop for FakeAVCeleb (with Frame Limiting and resume support) ---\n",
    "\n",
    "print(\"Starting FakeAVCeleb processing...\")\n",
    "print(f\"Frame selection policy: MIN {MIN_FRAMES_PER_VIDEO}, MAX {MAX_FRAMES_PER_VIDEO} (based on motion threshold {motion_threshold})\")\n",
    "\n",
    "# Counters for final summary\n",
    "total_videos_processed = 0\n",
    "total_videos_skipped = 0\n",
    "total_videos_already = 0\n",
    "\n",
    "for source_folder, label in SOURCE_FOLDERS:\n",
    "    # e.g., source_folder = \"RealVideo-RealAudio\", label = \"0_real\"\n",
    "    \n",
    "    input_dir = input_base_dir / source_folder\n",
    "    # The output_dir is now the final destination folder, e.g., E:\\Processed_FakeAVCeleb\\0_real\n",
    "    output_dir = output_base_dir / label\n",
    "    \n",
    "    if not input_dir.exists():\n",
    "        print(f\"Warning: Source directory not found. Skipping: {input_dir}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"--- Processing source folder: {source_folder} (Saving to {output_dir}) ---\")\n",
    "    \n",
    "    # Iterate through Ethnicity folders (e.g., \"African\", \"Asian (East)\")\n",
    "    for ethnicity_dir in input_dir.iterdir():\n",
    "        if not ethnicity_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        # Iterate through Gender folders (e.g., \"men\", \"women\")\n",
    "        for gender_dir in ethnicity_dir.iterdir():\n",
    "            if not gender_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            # Iterate through ID folders (e.g., \"id00076\")\n",
    "            for id_dir in gender_dir.iterdir():\n",
    "                if not id_dir.is_dir():\n",
    "                    continue\n",
    "                    \n",
    "                # We are now in the folder containing the videos\n",
    "                # (e.g., .../FakeVideo-RealAudio/African/men/id00076/)\n",
    "                \n",
    "                # Iterate through all video files in this folder\n",
    "                for video_file in id_dir.iterdir():\n",
    "                    # Check if it's a video file we can process\n",
    "                    if video_file.is_file() and video_file.suffix.lower() in video_extensions:\n",
    "\n",
    "                        # Build the unique video name and output folder for resume checks\n",
    "                        unique_video_name = f\"{id_dir.name}_{video_file.stem}\"\n",
    "                        video_output_folder = (output_dir / unique_video_name)\n",
    "\n",
    "                        # If video_output_folder already exists and contains enough frames, skip it\n",
    "                        if video_output_folder.exists():\n",
    "                            existing_files = list(video_output_folder.glob('*.png'))\n",
    "                            if len(existing_files) >= MIN_FRAMES_PER_VIDEO:\n",
    "                                print(f\"Already processed, skipping: {video_file}\")\n",
    "                                total_videos_already += 1\n",
    "                                continue\n",
    "\n",
    "                        # Print the full path of the video being processed so you can see progress\n",
    "                        print(f\"Processing video: {video_file}\")\n",
    "\n",
    "                        # Step 1: Sample frames\n",
    "                        frames = sample_frames_with_optical_flow(video_file, motion_threshold)\n",
    "                        \n",
    "                        # --- NEW: Check MIN frame count ---\n",
    "                        if len(frames) < MIN_FRAMES_PER_VIDEO:\n",
    "                            print(f\"Skipping {video_file.name}: only {len(frames)} frames. (Min: {MIN_FRAMES_PER_VIDEO})\")\n",
    "                            total_videos_skipped += 1\n",
    "                            continue # Skip this video\n",
    "                            \n",
    "                        # Step 2: Align faces\n",
    "                        faces = align_and_crop_faces(frames, output_face_size)\n",
    "                        \n",
    "                        # --- NEW: Check MIN face count (in case some frames fail detection) ---\n",
    "                        if len(faces) < MIN_FRAMES_PER_VIDEO:\n",
    "                            print(f\"Skipping {video_file.name}: only {len(faces)} faces detected. (Min: {MIN_FRAMES_PER_VIDEO})\")\n",
    "                            total_videos_skipped += 1\n",
    "                            continue # Skip this video\n",
    "                        \n",
    "                        # --- NEW: Apply MAX frame cap ---\n",
    "                        if len(faces) > MAX_FRAMES_PER_VIDEO:\n",
    "                            faces = faces[:MAX_FRAMES_PER_VIDEO] # Take only the first MAX_FRAMES\n",
    "                        \n",
    "                        # Step 3: Save frames in per-video folder\n",
    "                        save_processed_frames(faces, output_dir, unique_video_name)\n",
    "                        print(f\"Saved {len(faces)} faces for {unique_video_name} -> {video_output_folder}\")\n",
    "                        total_videos_processed += 1\n",
    "\n",
    "print(\"\\nAll processing complete.\")\n",
    "print(f\"Total videos processed and saved: {total_videos_processed}\")\n",
    "print(f\"Total videos skipped (insufficient frames): {total_videos_skipped}\")\n",
    "print(f\"Total videos already processed and skipped: {total_videos_already}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
